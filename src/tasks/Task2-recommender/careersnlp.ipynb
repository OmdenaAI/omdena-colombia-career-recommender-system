{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install nltk==3.4\n!pip install yellowbrick -U","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:18:06.629581Z","iopub.execute_input":"2021-11-05T15:18:06.630378Z","iopub.status.idle":"2021-11-05T15:18:25.412892Z","shell.execute_reply.started":"2021-11-05T15:18:06.630230Z","shell.execute_reply":"2021-11-05T15:18:25.411435Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import base64\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Plotly imports\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\npy.init_notebook_mode(connected=True)\n\n# Other imports\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation\nimport json\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nimport string\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-05T15:28:46.632825Z","iopub.execute_input":"2021-11-05T15:28:46.633199Z","iopub.status.idle":"2021-11-05T15:28:46.821588Z","shell.execute_reply.started":"2021-11-05T15:28:46.633164Z","shell.execute_reply":"2021-11-05T15:28:46.820756Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"References :\n1. https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial\n2. https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n3. https://github.com/ElizaLo/NLP-Natural-Language-Processing","metadata":{}},{"cell_type":"code","source":"df = pd.read_json('../input/careerdb/database.json')","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:23:38.114013Z","iopub.execute_input":"2021-11-05T15:23:38.114592Z","iopub.status.idle":"2021-11-05T15:23:38.657372Z","shell.execute_reply.started":"2021-11-05T15:23:38.114547Z","shell.execute_reply":"2021-11-05T15:23:38.656071Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.rename(columns={'pregrado':'undergraduate',\n                   'Universidad':'university',\n                   'detalles':'details',\n                   'Descripcion':'description',\n                  'Registro calificado':'Qualified record',\n                  'Nivel de formación':'Level of Education',\n                  'Tipo de formación':'Type of training',\n                  'Título otorgado':'Title awarded',\n                  'Modalidad':'Modality',\n                  'Duración':'Duration',\n                  'Créditos':'Credits',\n                  'Ciudad':'Town'},inplace=True)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:23:39.052538Z","iopub.execute_input":"2021-11-05T15:23:39.052940Z","iopub.status.idle":"2021-11-05T15:23:39.092202Z","shell.execute_reply.started":"2021-11-05T15:23:39.052897Z","shell.execute_reply":"2021-11-05T15:23:39.091332Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:23:41.526228Z","iopub.execute_input":"2021-11-05T15:23:41.526813Z","iopub.status.idle":"2021-11-05T15:23:41.558654Z","shell.execute_reply.started":"2021-11-05T15:23:41.526775Z","shell.execute_reply":"2021-11-05T15:23:41.557497Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.undergraduate.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:23:42.138191Z","iopub.execute_input":"2021-11-05T15:23:42.138574Z","iopub.status.idle":"2021-11-05T15:23:42.151701Z","shell.execute_reply.started":"2021-11-05T15:23:42.138539Z","shell.execute_reply":"2021-11-05T15:23:42.150189Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Delete .....\ndf1 = df[(df['undergraduate'].str.find('especializacion')==-1) &\n    (df['undergraduate'].str.find('maestria')==-1) & \n    (df['undergraduate'].str.find('doctorado')==-1) & \n    (df['undergraduate'].str.find('tecnologia')==-1)]\n\ndf1.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:23:43.039185Z","iopub.execute_input":"2021-11-05T15:23:43.039535Z","iopub.status.idle":"2021-11-05T15:23:43.077591Z","shell.execute_reply.started":"2021-11-05T15:23:43.039498Z","shell.execute_reply":"2021-11-05T15:23:43.076347Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df1.undergraduate.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:23:45.492861Z","iopub.execute_input":"2021-11-05T15:23:45.493562Z","iopub.status.idle":"2021-11-05T15:23:45.505038Z","shell.execute_reply.started":"2021-11-05T15:23:45.493516Z","shell.execute_reply":"2021-11-05T15:23:45.503686Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def replace_str(x):\n    if (str(x).find('administracion') != -1) or (str(x).find('contaduria') != -1) \\\n    or (str(x).find('economia') != -1) or (str(x).find('negocios') != -1) \\\n    or (str(x).find('mercadeo') != -1) or (str(x).find('finanzas') != -1) \\\n    or (str(x).find('publicidad') != -1) or (str(x).find('comercio') != -1):\n        return 'BUSINESS'\n    elif str(x).find('ingenieria') != -1:\n        return 'ENGINEERING'\n    elif (str(x).find('derecho') != -1)   or (str(x).find('psico') != -1) \\\n    or (str(x).find('comunicacion') != -1) or (str(x).find('social') != -1) \\\n    or (str(x).find('pedagogia') != -1) or (str(x).find('filosofia') != -1) \\\n    or (str(x).find('educacion infantil') != -1) or (str(x).find('teologia') != -1) \\\n    or (str(x).find('antropologia') != -1) or (str(x).find('ciencia politica') != -1) \\\n    or (str(x).find('historia') != -1) or (str(x).find('sociologia') != -1) \\\n    or (str(x).find('literatura') != -1) or (str(x).find('ciencias politicas') != -1) \\\n    or (str(x).find('relaciones internacionales') != -1):\n        return 'HUMANITIES AND SOCIAL SCIENCE'\n    elif (str(x).find('medicina') != -1) or (str(x).find('enfermeria') != -1) \\\n    or (str(x).find('odontologia') != -1) or (str(x).find('salud en el trabajo') != -1) \\\n    or (str(x).find('quirurgica') != -1) :\n        return 'HEALTH & MEDICINE'\n    elif (str(x).find('educacion fisica') != -1) or (str(x).find('fisioterapia') != -1):\n        return 'SPORTS AND PHYSICAL TRAIN'\n    elif (str(x).find('arquitectura') != -1) or (str(x).find('music') != -1) \\\n    or (str(x).find('diseño') != -1) or (str(x).find('artes') != -1) \\\n    or (str(x).find('fotografia') != -1):\n        return 'ARTS AND DESIGN'\n    elif (str(x).find('matematicas') != -1) or (str(x).find('fisica') != -1) \\\n    or (str(x).find('estadistica') != -1) or (str(x).find('biologia') != -1) \\\n    or (str(x).find('ciencias naturales') != -1) or (str(x).find('quimica') != -1) :\n        return 'MATH AND PHYSICAL SCIENCES'\n    else:\n        return 'OTHER'","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:23:47.559753Z","iopub.execute_input":"2021-11-05T15:23:47.560081Z","iopub.status.idle":"2021-11-05T15:23:47.582661Z","shell.execute_reply.started":"2021-11-05T15:23:47.560048Z","shell.execute_reply":"2021-11-05T15:23:47.581748Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df1['final_cat'] = df1['undergraduate'].apply(replace_str)\ndf1['final_cat'].value_counts(normalize = True)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:23:49.693303Z","iopub.execute_input":"2021-11-05T15:23:49.693980Z","iopub.status.idle":"2021-11-05T15:23:49.713068Z","shell.execute_reply.started":"2021-11-05T15:23:49.693933Z","shell.execute_reply":"2021-11-05T15:23:49.712180Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Droping OTHER Undergraduate\ndf1 = df1[df1['final_cat'] != 'OTHER']\n# Dropping void Description\ndf1= df1[df1['description']!='']\n# replacing big space\ndf1['description'] = df1['description'].str.strip('').replace('  ', ' ')","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:23:51.889302Z","iopub.execute_input":"2021-11-05T15:23:51.889950Z","iopub.status.idle":"2021-11-05T15:23:51.902641Z","shell.execute_reply.started":"2021-11-05T15:23:51.889908Z","shell.execute_reply":"2021-11-05T15:23:51.901509Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df1.head(2)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:23:52.959531Z","iopub.execute_input":"2021-11-05T15:23:52.959887Z","iopub.status.idle":"2021-11-05T15:23:52.980561Z","shell.execute_reply.started":"2021-11-05T15:23:52.959850Z","shell.execute_reply":"2021-11-05T15:23:52.979136Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"data = [go.Bar(\n            x = df1.final_cat.unique(),\n            y = df1.final_cat.value_counts().values,\n            marker= dict(colorscale='Jet',\n                         color = df1.final_cat.value_counts().values\n                        ),\n            text='Text entries attributed to Final Category'\n    )]\n\nlayout = go.Layout(\n    title='Target variable distribution'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:23:54.034636Z","iopub.execute_input":"2021-11-05T15:23:54.035585Z","iopub.status.idle":"2021-11-05T15:23:55.470356Z","shell.execute_reply.started":"2021-11-05T15:23:54.035496Z","shell.execute_reply":"2021-11-05T15:23:55.469626Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"all_words = df1['undergraduate'].str.split(expand=True).unstack().value_counts()\ndata = [go.Bar(\n            x = all_words.index.values[2:50],\n            y = all_words.values[2:50],\n            marker= dict(colorscale='Jet',\n                         color = all_words.values[2:100]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 (Uncleaned) Word frequencies in the Undergraduate'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:23:55.555274Z","iopub.execute_input":"2021-11-05T15:23:55.555806Z","iopub.status.idle":"2021-11-05T15:23:55.640482Z","shell.execute_reply.started":"2021-11-05T15:23:55.555766Z","shell.execute_reply":"2021-11-05T15:23:55.639210Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"all_words = df1['description'].str.split(expand=True).unstack().value_counts()\ndata = [go.Bar(\n            x = all_words.index.values[2:50],\n            y = all_words.values[2:50],\n            marker= dict(colorscale='Jet',\n                         color = all_words.values[2:100]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 (Uncleaned) Word frequencies in the Description'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:27:36.715079Z","iopub.execute_input":"2021-11-05T15:27:36.715437Z","iopub.status.idle":"2021-11-05T15:27:36.875235Z","shell.execute_reply.started":"2021-11-05T15:27:36.715398Z","shell.execute_reply":"2021-11-05T15:27:36.874030Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"all_words = df1['university'].str.split(expand=True).unstack().value_counts()\ndata = [go.Bar(\n            x = all_words.index.values[2:50],\n            y = all_words.values[2:50],\n            marker= dict(colorscale='Jet',\n                         color = all_words.values[2:100]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 (Uncleaned) Word frequencies in the University'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:27:37.067871Z","iopub.execute_input":"2021-11-05T15:27:37.068231Z","iopub.status.idle":"2021-11-05T15:27:37.153603Z","shell.execute_reply.started":"2021-11-05T15:27:37.068193Z","shell.execute_reply":"2021-11-05T15:27:37.152199Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"stopwords_new = ['universidad','programa',\n                 'formacion','desarrollo','profesionales',\n                 'colombia', 'estudiante', 'quindio', 'cooperativa', 'santo', 'tomas',\n                 'sergio','arboleda','pontificia','bolivariana']","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:27:38.178560Z","iopub.execute_input":"2021-11-05T15:27:38.178894Z","iopub.status.idle":"2021-11-05T15:27:38.184659Z","shell.execute_reply.started":"2021-11-05T15:27:38.178861Z","shell.execute_reply":"2021-11-05T15:27:38.183485Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"nltk.download('punkt')\nnltk.download('stopwords')\nspanish_stopwords = stopwords.words('spanish')\nspanish_stopwords.extend(stopwords_new)\n\ndef tokenize(sentence):\n    return [token for token in nltk.word_tokenize(sentence)]\ndef remove_stopwords(sentence):\n    return [token for token in nltk.word_tokenize(sentence) if (token.lower() not in spanish_stopwords) and (token.lower() !=' ') and (token not in string.punctuation)]","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:27:41.312986Z","iopub.execute_input":"2021-11-05T15:27:41.313362Z","iopub.status.idle":"2021-11-05T15:27:41.866086Z","shell.execute_reply.started":"2021-11-05T15:27:41.313316Z","shell.execute_reply":"2021-11-05T15:27:41.865223Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df1['token'] = df1['description'].apply(lambda x: tokenize(x))\ndf1['token_no_stopwords'] = df1['description'].apply(lambda x: remove_stopwords(x))\ndf1['bigram'] = df1['token_no_stopwords'].apply(lambda x: list(ngrams(x, 2)))","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:27:55.567055Z","iopub.execute_input":"2021-11-05T15:27:55.567802Z","iopub.status.idle":"2021-11-05T15:27:57.430947Z","shell.execute_reply.started":"2021-11-05T15:27:55.567749Z","shell.execute_reply":"2021-11-05T15:27:57.429457Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df1.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:28:16.144544Z","iopub.execute_input":"2021-11-05T15:28:16.144946Z","iopub.status.idle":"2021-11-05T15:28:16.231852Z","shell.execute_reply.started":"2021-11-05T15:28:16.144907Z","shell.execute_reply":"2021-11-05T15:28:16.230071Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(stop_words=spanish_stopwords)\nX = vectorizer.fit_transform(df1['description'])\n\nmodel = KMeans( init='k-means++', max_iter=400, random_state=2021,)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:45:25.885197Z","iopub.execute_input":"2021-11-05T15:45:25.885964Z","iopub.status.idle":"2021-11-05T15:45:25.977616Z","shell.execute_reply.started":"2021-11-05T15:45:25.885914Z","shell.execute_reply":"2021-11-05T15:45:25.976625Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from yellowbrick.cluster import KElbowVisualizer\n\n# k is range of number of clusters.\nvisualizer = KElbowVisualizer(model, k=(2,10), timings= True,)\nvisualizer.fit(X)        # Fit data to visualizer\nvisualizer.show()        # Finalize and render figure\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:45:29.146134Z","iopub.execute_input":"2021-11-05T15:45:29.146416Z","iopub.status.idle":"2021-11-05T15:45:36.190247Z","shell.execute_reply.started":"2021-11-05T15:45:29.146387Z","shell.execute_reply":"2021-11-05T15:45:36.189221Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"visualizer = KElbowVisualizer(model, k=(2,10), metric='silhouette', timings= True)\nvisualizer.fit(X)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:45:41.996510Z","iopub.execute_input":"2021-11-05T15:45:41.996793Z","iopub.status.idle":"2021-11-05T15:45:48.730969Z","shell.execute_reply.started":"2021-11-05T15:45:41.996763Z","shell.execute_reply":"2021-11-05T15:45:48.729932Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"n_clusters = 7\n\ndef get_clusters_top_words(n_clusters):\n    model = KMeans(n_clusters, init='k-means++', max_iter=400, random_state=2021)\n    model.fit(X)\n    order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n    terms = vectorizer.get_feature_names()\n\n    for i in range(n_clusters):\n        print('Cluster %d:' % i),\n        for ind in order_centroids[i, :5]:\n            print(' %s' % terms[ind])\n\nget_clusters_top_words(6)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:46:13.762737Z","iopub.execute_input":"2021-11-05T15:46:13.763046Z","iopub.status.idle":"2021-11-05T15:46:14.644313Z","shell.execute_reply.started":"2021-11-05T15:46:13.763012Z","shell.execute_reply":"2021-11-05T15:46:14.643220Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"get_clusters_top_words(3)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:46:29.995665Z","iopub.execute_input":"2021-11-05T15:46:29.995956Z","iopub.status.idle":"2021-11-05T15:46:30.545144Z","shell.execute_reply.started":"2021-11-05T15:46:29.995927Z","shell.execute_reply":"2021-11-05T15:46:30.544187Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\nbigram_list = [pair for row in df1['token_no_stopwords'] for pair in ngrams(row, 2)]\nbigram = Counter(bigram_list).most_common()\nbigram = pd.DataFrame.from_records(bigram, columns=['gram', 'count'])\nbigram[:20]","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:48:44.039465Z","iopub.execute_input":"2021-11-05T15:48:44.039822Z","iopub.status.idle":"2021-11-05T15:48:44.104644Z","shell.execute_reply.started":"2021-11-05T15:48:44.039785Z","shell.execute_reply":"2021-11-05T15:48:44.103746Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"words = (df1['token_no_stopwords'].apply(lambda x: ' '.join(x))).str.cat(sep=' ').split()\nCounter(words).most_common(50)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:49:04.886918Z","iopub.execute_input":"2021-11-05T15:49:04.887723Z","iopub.status.idle":"2021-11-05T15:49:04.909923Z","shell.execute_reply.started":"2021-11-05T15:49:04.887682Z","shell.execute_reply":"2021-11-05T15:49:04.908695Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import re\ndata = df1['token_no_stopwords'].str.join(' ').values.tolist()\ndata = [re.sub('\\s+', ' ', sent) for sent in data] # Remove new line characters\ndata = [re.sub(\"\\'\", \"\", sent) for sent in data] # Remove distracting single quotes","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:49:23.731915Z","iopub.execute_input":"2021-11-05T15:49:23.732916Z","iopub.status.idle":"2021-11-05T15:49:23.765086Z","shell.execute_reply.started":"2021-11-05T15:49:23.732875Z","shell.execute_reply":"2021-11-05T15:49:23.764302Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\n# spacy for lemmatization\nimport spacy\n\n# Plotting tools\nimport pyLDAvis\nimport pyLDAvis.gensim  # don't skip this","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:54:13.912328Z","iopub.execute_input":"2021-11-05T15:54:13.912674Z","iopub.status.idle":"2021-11-05T15:54:13.996011Z","shell.execute_reply.started":"2021-11-05T15:54:13.912642Z","shell.execute_reply":"2021-11-05T15:54:13.995165Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def sentence_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndata_words = list(sentence_to_words(data))\nprint(data_words[:1])","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:50:09.486940Z","iopub.execute_input":"2021-11-05T15:50:09.488091Z","iopub.status.idle":"2021-11-05T15:50:09.686869Z","shell.execute_reply.started":"2021-11-05T15:50:09.488048Z","shell.execute_reply":"2021-11-05T15:50:09.685841Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"# Creating Bigram and Trigram Models","metadata":{}},{"cell_type":"code","source":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n\n# Faster way to get a sentence clubbed as a trigram/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# See trigram example\nprint(trigram_mod[bigram_mod[data_words[0]]])","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:50:18.375497Z","iopub.execute_input":"2021-11-05T15:50:18.375968Z","iopub.status.idle":"2021-11-05T15:50:18.719151Z","shell.execute_reply.started":"2021-11-05T15:50:18.375932Z","shell.execute_reply":"2021-11-05T15:50:18.718263Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"# Remove Stopwords, Make Bigrams and Lemmatize\n\nThe bigrams model is ready. Let’s define the functions to remove the stopwords, make bigrams and lemmatization and call them sequentially.","metadata":{}},{"cell_type":"code","source":"def remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in spanish_stopwords] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:50:31.771871Z","iopub.execute_input":"2021-11-05T15:50:31.772275Z","iopub.status.idle":"2021-11-05T15:50:31.779348Z","shell.execute_reply.started":"2021-11-05T15:50:31.772195Z","shell.execute_reply":"2021-11-05T15:50:31.778015Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https://spacy.io/api/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:50:37.959747Z","iopub.execute_input":"2021-11-05T15:50:37.960476Z","iopub.status.idle":"2021-11-05T15:50:37.967024Z","shell.execute_reply.started":"2021-11-05T15:50:37.960407Z","shell.execute_reply":"2021-11-05T15:50:37.965928Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"!python -m spacy download es_core_news_sm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import es_core_news_sm\n\n# Remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n\n# Form Bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = es_core_news_sm.load()\n\n# Do lemmatization\ndata_lemmatized = lemmatization(data_words_bigrams)\n\nprint(data_lemmatized[:1])","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:51:21.174254Z","iopub.execute_input":"2021-11-05T15:51:21.174636Z","iopub.status.idle":"2021-11-05T15:51:32.257221Z","shell.execute_reply.started":"2021-11-05T15:51:21.174595Z","shell.execute_reply":"2021-11-05T15:51:32.255918Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"# Create the Dictionary and Corpus needed for Topic Modeling","metadata":{}},{"cell_type":"code","source":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[:1])","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:51:46.335149Z","iopub.execute_input":"2021-11-05T15:51:46.335946Z","iopub.status.idle":"2021-11-05T15:51:46.429332Z","shell.execute_reply.started":"2021-11-05T15:51:46.335903Z","shell.execute_reply":"2021-11-05T15:51:46.428046Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:5]]","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:51:48.909624Z","iopub.execute_input":"2021-11-05T15:51:48.909900Z","iopub.status.idle":"2021-11-05T15:51:48.925773Z","shell.execute_reply.started":"2021-11-05T15:51:48.909870Z","shell.execute_reply":"2021-11-05T15:51:48.924657Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"# Building the Topic Model\n\n* We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n\n* Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior.\n\n* chunksize is the number of documents to be used in each training chunk. update_every determines how often the model parameters should be updated and passes is the total number of training passes.","metadata":{}},{"cell_type":"code","source":"# Build LDA model\nlda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, # corpus\n                                           id2word=id2word, # index to word\n                                           num_topics=7,  # \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:55:02.046810Z","iopub.execute_input":"2021-11-05T15:55:02.047197Z","iopub.status.idle":"2021-11-05T15:55:06.077261Z","shell.execute_reply.started":"2021-11-05T15:55:02.047161Z","shell.execute_reply":"2021-11-05T15:55:06.076305Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"# View the topics in LDA model","metadata":{}},{"cell_type":"code","source":"best_model = None\ntop_score = 0\nfor x in range(1,16):\n    print(f'Number of topics:{x}')\n    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n                                           id2word=id2word,\n                                           num_topics=x, \n                                           random_state=100,\n                                           update_every=1,\n                                           chunksize=100,\n                                           passes=10,\n                                           alpha='auto',\n                                           per_word_topics=True)\n    \n    # Compute Perplexity\n    print(f'Perplexity for {x} topics: {lda_model.log_perplexity(corpus)}')  # a measure of how good the model is. lower the better.\n\n    # Compute Coherence Score\n    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n    coherence_lda = coherence_model_lda.get_coherence()\n    print(f'Coherence Score for {x} topics: {coherence_lda} \\n')\n    if coherence_lda > top_score:\n        best_model = x\n        top_score = coherence_lda\nprint(f'\\nBest Results with {best_model} topics with a Coherence of {top_score}')","metadata":{"execution":{"iopub.status.busy":"2021-11-05T16:06:23.347922Z","iopub.execute_input":"2021-11-05T16:06:23.348293Z","iopub.status.idle":"2021-11-05T16:07:44.601818Z","shell.execute_reply.started":"2021-11-05T16:06:23.348253Z","shell.execute_reply":"2021-11-05T16:07:44.600664Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"* Perplexity:  -8.868750111308447\n\n* Coherence Score:  0.4369763457091124\n\n**There we have a coherence score of 0.43.**","metadata":{}},{"cell_type":"markdown","source":"# Compute Model Perplexity and Coherence Score\n\nModel perplexity and topic coherence provide a convenient measure to judge how good a given topic model is. In my experience, topic coherence score, in particular, has been more helpful.","metadata":{}},{"cell_type":"code","source":"# Compute Perplexity\nprint('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n\n# Compute Coherence Score\ncoherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:57:32.816930Z","iopub.execute_input":"2021-11-05T15:57:32.817612Z","iopub.status.idle":"2021-11-05T15:57:34.690246Z","shell.execute_reply.started":"2021-11-05T15:57:32.817565Z","shell.execute_reply":"2021-11-05T15:57:34.688364Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"# Visualize the topics-keywords using pyLDAvis","metadata":{}},{"cell_type":"code","source":"# Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\nvis","metadata":{"execution":{"iopub.status.busy":"2021-11-05T15:58:18.433343Z","iopub.execute_input":"2021-11-05T15:58:18.434930Z","iopub.status.idle":"2021-11-05T15:58:22.279876Z","shell.execute_reply.started":"2021-11-05T15:58:18.434846Z","shell.execute_reply":"2021-11-05T15:58:22.278116Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"# Building LDA Mallet Model","metadata":{}},{"cell_type":"code","source":"# download the Mallet Model\n!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n!unzip ./mallet-2.0.8.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LDA MALLET & GENSIM WRAPPER is Removed in VERSION 4.X of GENSIM","metadata":{}},{"cell_type":"markdown","source":"# How to find the optimal number of topics for LDA?","metadata":{}},{"cell_type":"code","source":"def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n    \"\"\"\n    Compute c_v coherence for various number of topics\n\n    Parameters:\n    ----------\n    dictionary : Gensim dictionary\n    corpus : Gensim corpus\n    texts : List of input texts\n    limit : Max num of topics\n\n    Returns:\n    -------\n    model_list : List of LDA topic models\n    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n    \"\"\"\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model = gensim.models.wrappers.LdaMallet('/content/mallet-2.0.8/bin/mallet', corpus=corpus, num_topics=num_topics, id2word=id2word)\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_values.append(coherencemodel.get_coherence())\n\n    return model_list, coherence_values","metadata":{"execution":{"iopub.status.busy":"2021-11-05T16:25:41.872543Z","iopub.execute_input":"2021-11-05T16:25:41.872910Z","iopub.status.idle":"2021-11-05T16:25:41.882696Z","shell.execute_reply.started":"2021-11-05T16:25:41.872861Z","shell.execute_reply":"2021-11-05T16:25:41.881452Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"%%time\n# Can take a long time to run.\nmodel_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=20, step=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show graph\nlimit=20; start=2; step=1;\nx = range(start, limit, step)\nplt.plot(x, coherence_values)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.xticks(np.arange(start, limit, step=step))\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the coherence scores\nfor m, cv in zip(x, coherence_values):\n    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimal_model = model_list[3]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finding the dominant topic in each sentence","metadata":{}},{"cell_type":"code","source":"def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n\n# Show\ndf_dominant_topic.head(10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the Keyword in the 5 topics\npprint(optimal_model.print_topics())\ndoc_lda = optimal_model[corpus]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Find the most representative document for each topic","metadata":{}},{"cell_type":"code","source":"df_final = df.merge(df_dominant_topic, how='inner', left_index=True, right_index=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final.pivot_table(index='final_cat', columns='Dominant_Topic', values='Document_No', aggfunc='count')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_final.pivot_table(index='undergraduate', columns='Dominant_Topic', values='Document_No', aggfunc='count').head(10)","metadata":{},"execution_count":null,"outputs":[]}]}